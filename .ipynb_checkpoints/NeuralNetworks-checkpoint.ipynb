{
 "metadata": {
  "name": "",
  "signature": "sha256:69db4683c7cd03787f29e4e24d30bfff24254f91a8f0f83c45f4583891483c41"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Benefits of Using Artificial Neural Networks </h1>\n",
      "\n",
      "Neural Networks are a class of algorithm that attempt to remedy the principal gaps between computational and biological systems. There is no such thing as \"a neural network\", rather the term describes a class of models with different implementations, one of the most simple of which- the perceptron- we'll be covering later in this talk. The major benefits the use of neural networks afford computer programmers and mathematicians are their self-organization and learning capability, capability to generalize from relatively small data sets, and fault tolerance. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Robot Example </h1>\n",
      "\n",
      "I think a regular question people have with these sorts of algorithms is what sort of problem space to which they are suited. Let's assume you have a simple robot with three sensors on the front right, three on the front left and two on the back.\n",
      "\n",
      "Furthermore, let's assume we're also dealing with some real value as an input $\\mathbb{I} \\in \\mathbb{R}$ to the sensor and make even simpler by assuming that this signal is binary with H = 0 meaning everything is ok and H = 1 alerting the robot to stop. Now what we want to do is take these signals and make the robot move effectively; a problem that can be described mathematically as $\\mathbb{f}$ : $\\mathbb{R}^8 \\in \\mathbb{B}^1$, or to put it in human we want some function that applies the signals to a robot activity.\n",
      "\n",
      "Now, we can either go and get coffee and take a walk and try to think very hard of what the most effective way of solving the problem is and of course in a simple problem like this it would not be very difficult to solve. This is the classical AI approach emblematized with artificial intelligence sysems like Deep Blue. Rules of behaving were encoded and then more rules were generated for effectively searching through and selecting through these rules. While these approaches are beneficial when applied to closed systems like chess, it is difficult to use them in order to solve real world problems.\n",
      "\n",
      "A more interesting way of solving this problem is creating some method for the robot to learn. Once again this example is extremely trivial, but a lot of things that we'd be interesting in having aritfiically intelligent systems do are messy and ambiguous and dynamic. You can't always come up with the rules in advance, and having a system that can come up with it's own most effective way of doing something is an enormous asset. \n",
      "\n",
      "Let's summarize exactly what we're going to do in this sort of scenario. We're going to take the robot and show it a variety of different situations and hopes that it learns to stop effectively. In this example what we're going to call a \"training sample\"  is a set of sensor values which we show to the robot and a set of actions we force it to perform. Thus more generally a training set can be understood to consist of a set of idealized inputs and a corresponding desired output. We call the inputs idealized because again ideally the training set would consist of data that emblematizes the whole range of data the system will encounter after training. \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Brief History of Neural Networks </h1>\n",
      "\n",
      "While a lot of people perceive neural networks as a relatively modern development, the construction of the first neural networks actually conincides with the invention of the first programmable computers in the 1940s. Two researchers, Warren McCulloch and Walter Pitts created models of neurological networks and demonstrated that even these simple models were able to compute elementary logic problems. While neural networks started as a sort of proof of concept of the then somewhat contentious neuron theory of the brain (i.e. that atomic units rather than a uniform and undifferentiated mass could be capable of producing what we perceive as a relatively organic or holistic or what have you perception of consciousness), any self-respecting neuroscientist today would bristle at the suggestion that neural networks, a rather cartoonish depiction of the complexity of the human brain, would have the ability to generate insight into biological processes.\n",
      "\n",
      "After McCulloch and Pitts there was the neuroscientist Donald Hebb. If any of you have taken elementary courses in biology and neuroscience you've probably heard the refrain that \"neurons that fire together wire together\". Or the less cute version:\n",
      "\n",
      ">\"The general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend >to become 'associated', so that activity in one facilitates activity in the other.\"- Donald Hebb\n",
      "\n",
      "This is an extremely important idea. In the same way Bernoulli and Maxwell introduced statistics to mechanics, and Darwin allowed for history in biology, Hebb's theory introduces the importance of probability in neurological reactions. As we'll see later this forms the theoretical backbone of neural networks.\n",
      "\n",
      "Hebb's theory of learning was found to have a biological basis by Eric Kandel's work in long-term potentiation. Kandel was able to demonstrate a physiological analogue to classical conditioning.\n",
      "\n",
      "Minsky went on to develop the first neural network that took advantage of Hebb's expansive theories on the neuron. The program, named SNARC didn't necessarily do anything but was capable of altering the weight of its connections.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Neural Networks and Graphs </h1>\n",
      "\n",
      "This presentation is intended to provide a mathematical, biological, and computational overview of artificial neural networks.\n",
      "\n",
      "A neural network consists of neurons (vertices) and connections (edges). Each connection is through the course of running the algorithm assigned a weight.\n",
      "\n",
      "This is to say that a neural network is what computer scientists and mathematicians refer to as a graph. For the neural networks we're reviewing, the graph is a directed, weighted, acyclic graph. In fact, neural networks are actually instances of Bayesian networks which are partially defined as directed and acyclic graphs. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> A Mathematical Definition of a Neural Network </h1>\n",
      "\n",
      "A neural network can be formalized as the mathematical triplet (N, V, $\\omega$) where N is the set of neurons, V is the set {(i,j) | i,j $\\in$ N} where (i, j) is an arbitrary connection between neurons $N_{i}$ and $N_{j}$ and $\\omega$ is the function $V \\rightarrow \\mathbb{R}$ that maps each (i,j) $\\in$ N to its weight.\n",
      "\n",
      "A weight can be positive or negative in the same way that a neural connection can either be excitatory or inhibitory. In general, given any neuron j, there will be a number of neurons that transfer their output to j.\n",
      "\n",
      "<h2> Propagation Function </h2>\n",
      "For this neuron j, we can formalize the above statement by saying that it is receiving the outputs $o_{i}....o_{n}$ from neurons $i_{1}, i_{2}....i_{n}$ and transforms them in regards to their weights $\\omega_{i,j}$ into the net scalar value $net_{j}$\n",
      "\n",
      "This can be expressed as a weighted sum:\n",
      "\n",
      "Let I = $i_{1}, i_{2}...i_{n} (\\forall z \\in {1...n}): \\exists \\omega_{i_{z},j}$\n",
      "\n",
      "$net_{j}$ = $\\sum(o_{i}\\omega_{i,j})$\n",
      "\n",
      "<h2> Activation Function </h2>\n",
      "Any given neuron has some activation at any given time (t). This activation serves as a kind of \"switch\" that determines with the input of the $net_{j}$ whether or not the neuron will fire. Let's define the activation of a neuron j as $a_{j}$.\n",
      "\n",
      "Each neuron j has a threshold value $\\Theta_{j}$ which marks the highest value a neuron has to reach before it fires. As long as $a_{j}$ is less than $\\Theta_{j}$ the neuron will not fire. Now, the threshold is often defined globally but can be defined in relation to a <b> bias neuron </b> that always fires 1. Doing it this way allows algorithms to represent neuron biases as connection weights which means that you can train the threshold at the same time as you're training the weights of the individiaul neurons.\n",
      "\n",
      "The Activation Function can be described as $$a_{j}(t) = f_{act}(net_{j}(t), a_{j}(t-1), \\omega_{j})$$\n",
      "\n",
      "As you might have noticed, this is just a description of what the activation at time t is being equal to some activation function to which we pass three parameters, which as of yet doesn't help us a whole hell of a lot. These three parameters are\n",
      "the net activation at time t (which we got from the previous function), the activation at the previous time interval, and the maxium threshhold of neuron j.\n",
      "\n",
      "To recapitulate one more time, because this is where things get sort of hairy, it \"transforms the network input as well as the preivous activaton state into a new activation state with the threshold playing an important role\". \n",
      "\n",
      "<h3> Logistic Sigmoid Activation Function </h3>\n",
      "The reason the previous function definition was so vague, is because an activation function is really a class of different mathematical functions that can be used to model neural activation. One we're going to focus on is: \n",
      "$$1/(1 + e^{-x})$$\n",
      "which maps to the range of values (0,1) and is a sigmoidal function or a function that when graphed takes on an S-like shape. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/logisticurve.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2> The Output Function </h2>\n",
      "Finally, the third and last function we need to go over. This function takes the activation state from the previous function and uses it to calculate the output value (which is passed to other neurons). \n",
      "\n",
      "$$f_{out}(a_{j}) = o_{j}$$\n",
      "\n",
      "The vast majority of times, this function is simply an identity function.\n",
      "\n",
      "\n",
      "A summary. The data input of a neuron which consists of the number of connections multiplied by the different weights of those connections which form a scalar value are summed by a propagation function and transformed into a net input. This net input is directed to an activation function, which transforms net input and sometimes old activation to new activation. Finally this new activation is transformed into an output by an output function transmitted to other neurons. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Network Topologies </h1>\n",
      "\n",
      "<h2> Feed-Forward versus Recurrent Neural Networks </h2>\n",
      "\n",
      "<b> Feed-Forward Neural Networks </b> allow signals to travel only one way from input to output, signals don't go backwards and the output of any layer doesn't affect the layer itself. \n",
      "<b> Feedback Neural Networks </b> have signla straveling in both directions which introduce loops into the network. Computations derived from earlier input are fed back into the network which means that the state of the network changes which introduces a sort of 'memory'. They're dynamic and complicated and gradually converge to an equilibrium state which stays stable until the input changes. \n",
      "\n",
      "We're going to focus on feedforward networks in this talk for the following reasons:\n",
      "\n",
      "<ul> Life is short and this talk is as well</ul>\n",
      "<ul> Multi-layered perceptrons, which are a type of feed-forward network are the most widely studied </ul>\n",
      "<ul> We're going to implement a simple perceptron which is a type of feed-forward network </ul>\n",
      "\n",
      "<h2> Feed-Forward Neural Networks </h2>\n",
      "\n",
      "Feed-forward networks are suited for modeling relationships between a set of predictor or input variables and response or output variables. They mimic the quintessential Psych 101 form of learning, namely reinforcement learning, where a set of inputs (a ringing bell) gradually get associated with a set of outcomes (dog food!). \n",
      "\n",
      "They consist of one <b> input layer </b>, n-number of <b> hidden layers </b> and an <b> output layer </b>, the neurons of the latter of which are often referrred to with the symbol $\\Omega$. In feed-forward networks, the only connection each layer has is to the subsequent layer to which they can either be <b> sparsely </b> or completely connected.  I sort of lied to you in the previous sentence: while neurons can only be connected to neurons of the following layer, some types of feed-forward networks allow <b> short-cut connections </b> and then only to the output layer.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Some Learning and Training Set Fundamentals </h1>\n",
      "\n",
      "There are a bunch of different ways a neural network can learn, some of these ways are:\n",
      "\n",
      "<ol type= \"1\">\n",
      "<li> developing new connections </li>\n",
      "<li> deleting existing connections </li>\n",
      "<li> changing connection weights </li>\n",
      "<li> changing the threshold values of neurons </li>\n",
      "<li> altering one of the three neuron functions </li>\n",
      "<li> developing new neurons </li>\n",
      "<li> deleting existing neurons </li>\n",
      "</ol>\n",
      "\n",
      "with number three being the most common. \n",
      "\n",
      "We let our neural network learn by constructing a computer algorithm that expresses rules for altering the connecting weights. \n",
      "\n",
      "There's several different ways to do this but lets go over the three major learning paradigms:\n",
      "\n",
      "<ul>\n",
      "<li> <b> unsupervised learning </b>: most externally valid method but is difficult to implemenet and not suitable for all problems. Training set only consists of input patterns and the networks tries to detect similarities and generate pattern classes. </li>\n",
      "<li> <b> reinforcement learning </b> network receives a logical or boolean value after execution which defines whether it was \"right\" or \"wrong\", training set consists of input patterns with the addition that after completing a given sequence a value is returned indicating to the network whether it is \"right\" or \"wrong\" and sometimes How right or wron it was </li>\n",
      "<li> <b> supervised learning </b> the training set consists of input patterns as well as their correct results in the form of the precise activation of the output layer. This gives us the benefit of being able to directly compare the output with the actual results and further enables us to compute an error vector. </li>\n",
      "</ul>\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> A Typical Feed-Forward Neural Network Workflow </h1>\n",
      "\n",
      "<ul> \n",
      "<li> <b> Entering </b> the input pattern (input neurons are activated </li>\n",
      "<li> <b> Forward propagation </b> of the input by the network generation of the output </li>\n",
      "<li> <b> Comparing </b> the output with the desire output, giving us the difference or error vector </li>\n",
      "<li> <b> Corrections </b>  are calculated and applied </li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "So we're delivering input in batches and consistently comparing the teaching output to the ideal output. This is a little confusing but lets stick to formal definitions and define the teaching output as the teaching <b> input </b> $t_{j}$ compared to the true output $y_{j}$, with $t_{j}$ forming the correct output of a training pattern, which consists of the component vector $p_{1}, p_{2}...p_{n}$ whose desired output is known. The set of training patterns P consists of the ordered pairs (p, t) or the training patterns with their output. \n",
      "\n",
      "We can form an error vector $E_{p}$ which consists of \n",
      "\n",
      "$$E_{p} =  \\left( \\begin{array}{ccc}\n",
      "t_{1}-y_{1}\\\\\n",
      "...\\\\\n",
      "t_{n}-y_{n} \\\\ \\end{array} \\right) $$\n",
      "\n",
      "Where the error scalar can be derived:\n",
      "$Err_{p} = 1/2(\\sum(t_{k} - y_{k})^2$ \n",
      "\n",
      "This might look a little scary, but it's just the differences between the teaching inputs (which are outputs of that particular training pattern) and the actual output.\n",
      "\n",
      "Now for a brief break from the theoretical. You generally want to divide the training set into one set to use for the actual training and then a verification set to test our progress. Let's say 60/40 with 60% of the data being put into the training set. \n",
      "\n",
      "How do you know you're done?\n",
      "\n",
      "When the error value eventually converges. Note: this process is always a little bit arbitrary and can be expressed with confidence intervals and detected with the shape of the graph.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Gradient Descent </h1>\n",
      "\n",
      "Gradient descent optimization techniques are used when we want to maximize or minimize a function. The gradient is a vector defined for any differentiable point of a function that points to the steepest ascent of that function. \n",
      "\n",
      "Because multivariate calculus I'm only going to do a high-level visual description of this optimization algorithm. Remember how you can compute absolute minima and maxima of a function by taking the derivative of a function and setting it equal to 0 and putting those values back in the function? Then you take the second derivative and if it's positive it's a maximum and if it's negative it's a minimum?\n",
      "\n",
      "Gradient descent is a generalization of that process for n-dimensional functions. Imagine a ball rolling around a function on a 3-dimensional plane. If that function is an error function then we want to find the place the ball gets stuck because that'll be where the error is minimized and the values we want the weight vector of our neural network to have. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/gradientdescent.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Back Propagation of Error </h1>\n",
      "\n",
      "So remember when we chose Fermi's function as our sigmoid for the activation function?\n",
      "\n",
      "You'll be happy we did because the derivative is super easy and nice and can be expressed\n",
      "\n",
      "$$f^{'}(x) = f(x)(1-f(x))$$\n",
      "\n",
      "What does that look like you ask? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/sigmoid.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It turns out the integral of a normal distribution is a sigmoid function and thus the derivative of Fermi's function is a nice bell shape. \n",
      "Now, we have the derivative of the sigmoid function which lets us know the behavior of the function at any given point (in this case $a_{k}$. \n",
      "\n",
      "Remember when we calculated the sum of the squared error as:\n",
      "$$Err_{p} = 1/2(\\sum(t_{k} - y_{k})^2$$\n",
      "\n",
      "We're going to use the derivative to compute the error as follows: $$\\delta_{k} = g^{'}(a_{k})(y_{k}-t_{k})$$\n",
      "\n",
      "then the change in weight or $\\Delta w_{i,j}$ as a batch process is equivalent to\n",
      "$$\\sum\\delta x_{i}$$\n",
      "(here we're assuming for the sake of simplicity that the learning rate is equal to 1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> The Perceptron </h1>\n",
      "Now that we have a little bit more of a shared vocabulary, we can define the perceptron as a type of feed-forward neural network with short-cut connections. The input layer of a perceptron has constant weights, but all of the other layers can and must have connection weights that change. \n",
      "\n",
      "We're going to implement a binary perceptron, so all neurons in the perceptron have a possible value of {0,1} and that output in conjunction with a binary threshold acts as a sort of logic gate or if-clause (fire IF activation is 1). The input neuron is an identity neuron that exactly forwards the information received. The next layer or layers are the actual training layers with the three functions we covered above. Finally, we have the output layer which is just an identity function (and thus equivalent) to the output of the training layer. \n",
      "\n",
      "We're going to deal with a single-layer perceptron, which CAN but does not have to have multiple output neurons. \n",
      "\n",
      "<h2> The Guarantee </h2>\n",
      "Now, the really cool thing about the Perceptron is that it comes with a guarantee. Namely, any problem that is <b> linearly seperable </b> is guaranteed to be solved (converge to a solution) in a finite amount of time! Take that one to the bank!\n",
      "\n",
      "Note: attuned readers will notice here the conditional statement regarding linear seperability which will turn out to be a <b> big </b> problem that severely limits the utility of the single-layer perceptron. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Python Implementation of Single Layer Perceptron </h1>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import random\n",
      "import os, subprocess\n",
      " \n",
      "class Perceptron:\n",
      "    def __init__(self, N):\n",
      "        # Random linearly separated data\n",
      "        xA,yA,xB,yB = [random.uniform(-1, 1) for i in range(4)]\n",
      "        self.V = np.array([xB*yA-xA*yB, yB-yA, xA-xB])\n",
      "        self.X = self.generate_points(N)\n",
      " \n",
      "    def generate_points(self, N):\n",
      "        X = []\n",
      "        for i in range(N):\n",
      "            x1,x2 = [random.uniform(-1, 1) for i in range(2)]\n",
      "            x = np.array([1,x1,x2])\n",
      "            s = int(np.sign(self.V.T.dot(x)))\n",
      "            X.append((x, s))\n",
      "        return X\n",
      " \n",
      "    def plot(self, mispts=None, vec=None, save=True):\n",
      "        image = plt.figure(figsize=(5,5))\n",
      "        plt.xlim(-1,1)\n",
      "        plt.ylim(-1,1)\n",
      "        V = self.V\n",
      "        a, b = -V[1]/V[2], -V[0]/V[2]\n",
      "        l = np.linspace(-1,1)\n",
      "        plt.plot(l, a*l+b, 'k-')\n",
      "        cols = {1: 'r', -1: 'b'}\n",
      "        for x,s in self.X:\n",
      "            plt.plot(x[1], x[2], cols[s]+'o')\n",
      "        if mispts:\n",
      "            for x,s in mispts:\n",
      "                plt.plot(x[1], x[2], cols[s]+'.')\n",
      "        if vec != None:\n",
      "            aa, bb = -vec[1]/vec[2], -vec[0]/vec[2]\n",
      "            plt.plot(l, aa*l+bb, 'g-', lw=2)\n",
      "        if save:\n",
      "            if not mispts:\n",
      "                plt.title('N = %s' % (str(len(self.X))))\n",
      "            else:\n",
      "                plt.title('N = %s with %s test points' \\\n",
      "                          % (str(len(self.X)),str(len(mispts))))\n",
      "            plt.savefig('p_N%s' % (str(len(self.X))), \\\n",
      "                        dpi=200, bbox_inches='tight')\n",
      " \n",
      "    def classification_error(self, vec, pts=None):\n",
      "        # Error defined as fraction of misclassified points\n",
      "        if not pts:\n",
      "            pts = self.X\n",
      "        M = len(pts)\n",
      "        n_mispts = 0\n",
      "        for x,s in pts:\n",
      "            if int(np.sign(vec.T.dot(x))) != s:\n",
      "                n_mispts += 1\n",
      "        error = n_mispts / float(M)\n",
      "        return error\n",
      " \n",
      "    def choose_miscl_point(self, vec):\n",
      "        # Choose a random point among the misclassified\n",
      "        pts = self.X\n",
      "        mispts = []\n",
      "        for x,s in pts:\n",
      "            if int(np.sign(vec.T.dot(x))) != s:\n",
      "                mispts.append((x, s))\n",
      "        return mispts[random.randrange(0,len(mispts))]\n",
      " \n",
      "    def pla(self, save=False):\n",
      "        # Initialize the weigths to zeros\n",
      "        w = np.zeros(3)\n",
      "        X, N = self.X, len(self.X)\n",
      "        it = 0\n",
      "        # Iterate until all points are correctly classified\n",
      "        while self.classification_error(w) != 0:\n",
      "            it += 1\n",
      "            # Pick random misclassified point\n",
      "            x, s = self.choose_miscl_point(w)\n",
      "            # Update weights\n",
      "            w += s*x\n",
      "            if save:\n",
      "                self.plot(vec=w)\n",
      "                plt.title('N = %s, Iteration %s\\n' \\\n",
      "                          % (str(N),str(it)))\n",
      "                plt.savefig('p_N%s_it%s' % (str(N),str(it)), \\\n",
      "                            dpi=200, bbox_inches='tight')\n",
      "        self.w = w\n",
      " \n",
      "    def check_error(self, M, vec):\n",
      "        check_pts = self.generate_points(M)\n",
      "        return self.classification_error(vec, pts=check_pts)\n",
      "\n",
      "p = Perceptron(20)\n",
      "p.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAFCCAYAAACNRnTmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCMABAhGAAj8f4R2na7pOg3RAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1065dfd50>"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    import math\n",
      "    import random\n",
      "\n",
      "    BIAS = -1\n",
      "\n",
      "\n",
      "    class Neuron:\n",
      "        def __init__(self, n_inputs ):\n",
      "            self.n_inputs = n_inputs\n",
      "            self.set_weights( [random.uniform(0,1) for x in range(0,n_inputs+1)] ) # +1 for bias weight\n",
      "\n",
      "        def sum(self, inputs ):\n",
      "            return sum(val*self.weights[i] for i,val in enumerate(inputs))\n",
      "\n",
      "        def set_weights(self, weights ):\n",
      "            self.weights = weights\n",
      "\n",
      "        def __str__(self):\n",
      "            return 'Weights: %s, Bias: %s' % ( str(self.weights[:-1]),str(self.weights[-1]) )\n",
      "\n",
      "    class NeuronLayer:\n",
      "        def __init__(self, n_neurons, n_inputs):\n",
      "            self.n_neurons = n_neurons\n",
      "            self.neurons = [Neuron( n_inputs ) for _ in range(0,self.n_neurons)]\n",
      "\n",
      "        def __str__(self):\n",
      "            return 'Layer:\\n\\t'+'\\n\\t'.join([str(neuron) for neuron in self.neurons])+''\n",
      "\n",
      "    class NeuralNetwork:\n",
      "        def __init__(self, n_inputs, n_outputs, n_neurons_to_hl, n_hidden_layers):\n",
      "            self.n_inputs = n_inputs\n",
      "            self.n_outputs = n_outputs\n",
      "            self.n_hidden_layers = n_hidden_layers\n",
      "            self.n_neurons_to_hl = n_neurons_to_hl\n",
      "\n",
      "           \n",
      "            self._create_network()\n",
      "            self._n_weights = None\n",
      "            \n",
      "\n",
      "        def _create_network(self):\n",
      "            if self.n_hidden_layers>0:\n",
      "                # create the first layer\n",
      "                self.layers = [NeuronLayer( self.n_neurons_to_hl,self.n_inputs )]\n",
      "\n",
      "                # create hidden layers\n",
      "                self.layers += [NeuronLayer( self.n_neurons_to_hl,self.n_neurons_to_hl ) for _ in range(0,self.n_hidden_layers)]\n",
      "\n",
      "                # hidden-to-output layer\n",
      "                self.layers += [NeuronLayer( self.n_outputs,self.n_neurons_to_hl )]\n",
      "            else:\n",
      "                # If we don't require hidden layers\n",
      "                self.layers = [NeuronLayer( self.n_outputs,self.n_inputs )]\n",
      "\n",
      "        def get_weights(self):\n",
      "            weights = []\n",
      "\n",
      "            for layer in self.layers:\n",
      "                for neuron in layer.neurons:\n",
      "                    weights += neuron.weights\n",
      "\n",
      "            return weights\n",
      "\n",
      "        @property\n",
      "        def n_weights(self):\n",
      "            if not self._n_weights:\n",
      "                self._n_weights = 0\n",
      "                for layer in self.layers:\n",
      "                    for neuron in layer.neurons:\n",
      "                        self._n_weights += neuron.n_inputs+1 # +1 for bias weight\n",
      "            return self._n_weights\n",
      "\n",
      "        def set_weights(self, weights ):\n",
      "            assert len(weights)==self.n_weights, \"Incorrect amount of weights.\"\n",
      "\n",
      "            stop = 0\n",
      "            for layer in self.layers:\n",
      "                for neuron in layer.neurons:\n",
      "                    start, stop = stop, stop+(neuron.n_inputs+1)\n",
      "                    neuron.set_weights( weights[start:stop] )\n",
      "            return self\n",
      "\n",
      "        def update(self, inputs ):\n",
      "            assert len(inputs)==self.n_inputs, \"Incorrect amount of inputs.\"\n",
      "\n",
      "            for layer in self.layers:\n",
      "                outputs = []\n",
      "                for neuron in layer.neurons:\n",
      "                    tot = neuron.sum(inputs) + neuron.weights[-1]*BIAS\n",
      "                    outputs.append( self.sigmoid(tot) )\n",
      "                inputs = outputs   \n",
      "            return outputs\n",
      "\n",
      "        def sigmoid(self, activation,response=1 ):\n",
      "            # the activation function\n",
      "            try:\n",
      "                return 1/(1+math.e**(-activation/response))\n",
      "            except OverflowError:\n",
      "                return float(\"inf\")\n",
      "\n",
      "        def __str__(self):\n",
      "            return '\\n'.join([str(i+1)+' '+str(layer) for i,layer in enumerate(self.layers)])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}